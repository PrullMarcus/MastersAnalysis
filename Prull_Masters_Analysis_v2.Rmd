---
title: "Prull_Master_Analysis"
author: "Marcus Prull"
date: "12/10/2024"
output: html_document
---


Loading in required packages for analysis
```{r setup, include=FALSE}

require(lubridate)#For making date column
require(cowplot)#For any plot griding 
require(dplyr)#Filtering data
require(ggplot2)#Making final plots
require(geosphere)#package for straight line distance calculations
require(sf)#packing to read my spatial objects, EX. My Neely Henry Shapefile
require(raster)#used for raster functions
require(terra)#spatial analyst functions
require(gdistance)#transition function package
require(adehabitatHR)#Homerange functions
require(adehabitatLT)#Need this for Brownian Bridge Movement Model
```

First we need to read in my files from github. First file being read into R is the tracking data master sheet followed by my shapefile of Neely Henry which will be utilized later. Additionally, I load in the sections SHP file which will be used later on for the main model. 
```{r}

TrackMaster = read.csv("https://github.com/PrullMarcus/MastersAnalysis/raw/main/TrackingDataMaster.csv")
NeelySHP = st_read("/vsicurl/https://github.com/PrullMarcus/MastersAnalysis/raw/main/SHP%20Files/NeelySHP_Feb2024_NAD_1983_2011.shp") 
NeelySections = st_read("/vsicurl/https://github.com/PrullMarcus/MastersAnalysis/raw/main/SHP%20Files/NeelySections.shp")

```

Here are both of the shapefiles plotted just to ensure they visually look correct. 
```{r}
plot(NeelySHP$geometry)
plot(NeelySections$geometry)#You can barely see the section boundaries but it does appear they are there. 
```

First I have some tidying up to do to make sure everything runs smoothly. 

Firstly, all the dates in the tracking data are in Month, Day, Year format with a separate column for each. I use the lubridate package to create a new "Date" column which will store all three of those into 1 column as a "date" format instead of just a numeric value. This will allow me to subtract dates from each other later which is necessary for movement values over time/rates. 
```{r}

Date = make_date(year = TrackMaster$Year, month = TrackMaster$Month, day = TrackMaster$Day)
TrackMaster = cbind(Date,TrackMaster)#binding the vector to the main csv
colnames(TrackMaster)[1]<-"Date"#Naming the date column "Date"
TrackMaster

```

# Manually Tracked Non-Tournament Individuals Movement

## Seasonal Movement Patterns

First, we will look at manual tracking movements. There is an indicator column ("ManuallyTracked") that indicates whether we manually tracked a fish or not. For every fish the first location is the tagging location, followed by each subsequent location, therefore we need fish with >1 observations. 
```{r}
Manuals = TrackMaster[TrackMaster$ManuallyTracked=="Y",]
Manuals = Manuals %>% group_by(RewardTag1) %>% filter (n()>2)
```

To get just non-tournament individuals we can filter the data further using the indicator column of "TournamentRelease." We need to remove all the locations that have a "Y" associated with them. We also need to make sure that because we excluded some locations, we still have 3+ observations of each fish. (2 observations AFTER its tagging location). 
```{r}
NT_Manuals_All = subset(Manuals, TournamentRelease != "Y")
NT_Manuals_All = NT_Manuals_All %>% group_by(RewardTag1) %>% filter (n()>2)
```

Here I setup vectors to be used by the loop to ensure that it loops over each tag number independently of one another. Additionally, I setup the empty containers for the results of the loops to go into. One container is the raw distance values between successive locations and the second container is the days elapsed between successive observed locations. 

The first double for-loop for these movements will be utilizing STRAIGHT LINE DISTANCE since it runs in only a few seconds and makes it easier to diagnose errors in the code. Later, we will run LEAST COST DISTANCE, which plots the shortest route between points while making sure the distance calculation is bounded by the lake. THe least cost distance code, however, takes much much longer to run (hours). 

From here on out, NT = "NonTournament", T = "TournamentRelease", SL = "Straight-line distance", and LC = "Least-cost distance."

```{r}
tag_nums = unique(NT_Manuals_All$RewardTag1)
n_fish = length(tag_nums)# of individual fish making up dataset

NT_Manuals_SL = numeric()
NT_Manuals_days = numeric()#only need to calculate this one time... So, this vector will not be included in the second loop
```

### Straight-Line Distances

First we will setup the loop to calculate STRAIGHT LINE DISTANCE between successive locations for my control individuals. 

```{r}
#Outside loop that selects tag-number 
for(i in 1:n_fish){
  
  new_tag = tag_nums[i]
  new_dat = subset(NT_Manuals_All,RewardTag1 == new_tag)
  tmax = nrow(new_dat)
  NT_Manuals_Dist = numeric(tmax)
  days = numeric(tmax)
  
  for(t in 2:tmax){
    
    dist_m = geosphere::distm(c(new_dat$Long[t],new_dat$Lat[t]),c(new_dat$Long[t-1],new_dat$Lat[t-1]))#
    NT_Manuals_Dist[t] = dist_m
    days[t] = as.numeric(new_dat$Date[t]-new_dat$Date[t-1])
  }
  NT_Manuals_SL = append(NT_Manuals_SL,NT_Manuals_Dist)
  NT_Manuals_days = append(NT_Manuals_days,days)
}

```

Here we just combine the output from the loop to the original dataset. 
```{r}
NT_Manuals_All = cbind(NT_Manuals_SL, NT_Manuals_All)
colnames(NT_Manuals_All)[1]<-"SL_M" #straight line_meters

NT_Manuals_All = cbind(NT_Manuals_days,NT_Manuals_All)
colnames(NT_Manuals_All)[1]<-"DaysFromLastLoc"

NT_Manuals_All
```
Now, because the first location is the original tagging location, the distance from the first to second record is essentially negligible. We didn't start tracking till almost 3 ish months after tagging. Therefore, the days elapsed and distance values from the first two rows for EACH fish is essentially negligible. Here I filter the data down to reflect that. 

Additionally, I add the movement rates as a new column to the data

```{r}
NT_Manuals_All$SL_M_D = NT_Manuals_All$SL_M/NT_Manuals_All$DaysFromLastLoc#Straight-line meters per day moved
NT_Manuals_All$SL_M_W = NT_Manuals_All$SL_M/(NT_Manuals_All$DaysFromLastLoc/7)#Straight-line Meters per week moved
```

### Least-cost (in-lake) distances

Now we can calculate the in lake distances for our manually tracked non-tournament fish which takes a substantial time to run on the computer and much more preparation. The only thing that will change is the inputs into the loop. To do this I need to convert the NeelyHenry shapefile into a raster which is then used to create a "cost surface" raster. The cost-surface raster is indicative of the water and excludes land. It then calculates the most conservative distance between successive points while staying within the bounds of the reservoir. 

Now we need to convert the shapefile into a vector, then a raster, and then a transition matrix which is then used by the least cost distance functions. 

```{r}
NeelyVec = vect(NeelySHP)
NeelyR = rast(NeelyVec, res = 0.00008)#res of 8.8 m grid cells, value of 1 is 1 "degree". that is why we need to use a big decimal to get it to meters. tried 00004 but it took way too long. 

NeelyRast = rasterize(NeelyVec,NeelyR)#value of 1 is Neely, NA value is land

NeelyRast[is.na(NeelyRast)] = 100

plot(NeelyRast)
```
Looks correct. The pixels with a value of "1" are Water and "100" is land. It does appear that there are some gaps in the raster which I don't really know how to fix. It does appear that the more fine-scale you make the raster the better (less gaps) there appears to be. But, if the calculation has to involve land at all, it will choose the most conservative route through these gaps due to its high cost (100)

Now, we have to convert this raster that we created into a different format which is used by the least-cost distance function. This will take a little bit of time to run. 
```{r}

NeelyRast = raster(NeelyRast)#Converts spatraster to "raster" which is a different form of raster that the transition function needs in order to work properly. 
NeelyTrans = transition(1/NeelyRast, transitionFunction = mean, directions = 8)

#Here we correct for corners
NeelyTrans = geoCorrection(NeelyTrans, type = 'c', multpl=F)

NeelyTransRast = raster(NeelyTrans)
plot(NeelyTransRast)

```

Before we run the least-cost distance loop we need to create a spatial object (points) from the lat/long columns from our control fish dataframe. The least cost function requires a spatial object as input into the calculation that has the same coordinate reference system as the Neely Shapefile. 


Here we convert our dataset to a spatial object.
```{r}
NT_Manuals_All = sf::st_as_sf(NT_Manuals_All, coords = c("Long","Lat"),crs = crs(NeelySHP))
```

Plotting of the data to make sure they show up together.
```{r}
plot(NeelyTransRast)
plot(NT_Manuals_All,add=T)
```

Here we do the same thing as before. The only thing that changes is the outputs are saved under a different name(LC = least cost) whereas before it was SL (straight-line). Additionally, the "distm" function is now changed to costDistance from the gdistance package in order to do least-cost distances (within lake distances). 
```{r}
Start = Sys.time()

tag_nums = unique(NT_Manuals_All$RewardTag1)#unique tag numbers
n_fish = length(tag_nums)#number of fish

NT_Manuals_LC = numeric()

for(i in 1:n_fish){
  
  new_tag = tag_nums[i]
  new_dat = subset(NT_Manuals_All,RewardTag1 == new_tag)
  tmax = nrow(new_dat)
  NT_Manuals_Dist = numeric(tmax)
  
  for(t in 2:tmax){
    
    dist_m = gdistance:::costDistance(NeelyTrans, fromCoords = sf::st_coordinates(new_dat[(t-1),]),
                                                  toCoords = sf::st_coordinates(new_dat[t,]))
    NT_Manuals_Dist[t] = dist_m
    
  }
  NT_Manuals_LC = append(NT_Manuals_LC,NT_Manuals_Dist)
}
NT_Manuals_LC

End = Sys.time()
difftime(End,Start, units = "mins")
```

Adding the output to our dataset and calculating movement speeds just as we did before.
```{r}

NT_Manuals_All = cbind(NT_Manuals_All,NT_Manuals_LC)
colnames(NT_Manuals_All)[colnames(NT_Manuals_All) == "NT_Manuals_LC"] <- "LC_M"#renaming column to match Straight-line column name. 
NT_Manuals_All$LC_M_D = NT_Manuals_All$LC_M/NT_Manuals_All$DaysFromLastLoc
NT_Manuals_All$LC_M_W = NT_Manuals_All$LC_M/(NT_Manuals_All$DaysFromLastLoc/7)

```


Just a raw plot to show differences in SL vs LC movement rates. Appears to be an outlier that needs to be addressed. 
```{r}
plot(NT_Manuals_All$LC_M_W~NT_Manuals_All$SL_M_W,ylim=c(0,15000),xlim=c(0,15000))
abline(a=0,b=1)

plot(NT_Manuals_All$LC_M_W~NT_Manuals_All$SL_M_W,ylim=c(0,5000),xlim=c(0,5000))
abline(a=0,b=1)
```

## Movement Rates by species
Now I want to look at how movement rates differ by species across the year. Traditionally, Alabama Bass (ALB) are thought of as a more free-range fish that will suspend out in the water column chasing bait. Converseley, Largemouth Bass (LMB) are typically thought to be much more sedentary. 

Here is a graph showing the differences in movement distribution by time. The x axis will be in month/year format and the y is movement rate (meters/week). 

To do this we first need to separate out fish that are alive. Additionally we need to get rid of their first two observations. The first observation is the tagging location and then there is a long time gap between that location and their first tracking location. 

```{r}
NT_Manuals_Alives = subset(NT_Manuals_All, GroupMovement == "A")
NT_Manuals_Alives$MonthYear = strftime(NT_Manuals_Alives$Date, format = "%Y-%m")
NT_Manuals_Alives = NT_Manuals_Alives %>% group_by(RewardTag1) %>% filter (n()>2)#making sure we have 3 alive locations... (we need this before but it included dead coding locations as well). 
NT_Manuals_Alives = NT_Manuals_Alives %>% group_by(RewardTag1) %>% slice(3:n())
```

```{r}
#Boxplot comparing species movement over months. 
ggplot(NT_Manuals_Alives,aes(as.factor(MonthYear),SL_M_D))+
      geom_boxplot(aes(fill=Species))+
      coord_cartesian(ylim=c(0,1600))+
      theme_classic(base_size=12)+
      scale_y_continuous(expand=c(0.013,0))+
      labs(x="Month-Year",y="Movement Speed (meters/week)")
```
Obviously this is hard to look at as there are many outliers across most of the months. Here I shrink down the plot and remove outliers to get a better look at what is going on with our movement across months.

Trying something to get the year and month to not overlap each other (Separate tracking data by calendar year???)

3 panels (2022, 2023, 2024)

```{r}
ggplot(NT_Manuals_Alives,aes(as.factor(MonthYear),SL_M_D))+
      geom_boxplot(aes(fill=Species),outlier.color=NA)+
      coord_cartesian(ylim=c(0,250))+
      theme_classic(base_size=12)+
      scale_y_continuous(expand=c(0.013,0))+
      labs(x="Month-Year",y="Movement Speed (meters/week)")
```
It appears that LMB tended to have a wider distribution of movement than ALB across most months. Additionally, it appears that the median movement values were higher for nearly every month as well.





# Tournament Dispersal Analysis
Now we want to look at tournament dispersal movement for fish displaced by tournament. To calculate their movement rates we will do a similar loop as before just with different data. The only difference being that there is no lag between the first and second location like there was for the Non-tournament fish. Thus, we only need individuals with 2 or more locations to create a meaningful movement value. 

First we need to isolate our alive tournament fish to calculate our values. 

```{r}
T_Manuals_All = subset(Manuals, TournamentRelease == "Y")
T_Manuals_All = T_Manuals_All %>% group_by(RewardTag1) %>% filter(n()>1)
```

Here I make a date column which will be used to calculate days elapsed between step lengths as well as days elapsed since the fish was released at a tournament boat launch. 
```{r}
Dates = make_date(year=T_Manuals_All$Year,month=T_Manuals_All$Month,day=T_Manuals_All$Day)
T_Manuals_All = cbind(Dates,T_Manuals_All)
colnames(T_Manuals_All)[1] <- "Date"
```

I  created a spatial points class from the lat longs for the displaced fish and then plotted them on the Neely shp file to make sure they show up on the map.
```{r}
T_Manuals_All = sf::st_as_sf(T_Manuals_All, coords = c("Long","Lat"),crs = crs(NeelySHP))#making spatial object out of lat/long columns

plot(NeelySHP$geometry)
plot(T_Manuals_All, add = T)
```

Here is the loop that calculates each given fishes proximity to the ramp over time. Additionally, it calculates the movement rates of these fish over time. Both are stored in separate vectors. The proximity to the ramp is calculated by just doing a least cost distance of each location to each fish's starting location (the boat ramp). The movement rates/speeds are calculated in the same manner as they were for the control (non translocated fish) by calculating the step length between successive locations and then dividing by the time elapsed between the steps.Additionally, we add a vector that is WEEKS ELAPSED post capture to help with the first goal. 
```{r}
Start = Sys.time()

tag_nums = unique(T_Manuals_All$RewardTag1)#unique tag numbers
n_fish = length(tag_nums)#number of fish

T_LC_Prox = numeric()
T_LC_Step = numeric()
T_Days_Step = numeric()
T_Days_Rel = numeric()

for(i in 1:n_fish){
  
  new_tag = tag_nums[i]
  new_dat = subset(T_Manuals_All,RewardTag1 == new_tag)
  tmax = nrow(new_dat)
  Dispersal_prox = numeric(tmax)
  Dispersal_step = numeric(tmax)
  Days_Step = numeric(tmax)
  Days_Rel = numeric(tmax)
  
  for(t in 2:tmax){
    
    prox_m = gdistance:::costDistance(NeelyTrans, fromCoords = sf::st_coordinates(new_dat[1,]),
                                                  toCoords = sf::st_coordinates(new_dat[t,]))
    step_m = gdistance::costDistance(NeelyTrans, fromCoords = sf::st_coordinates(new_dat[(t-1),]),
                                                  toCoords = sf::st_coordinates(new_dat[t,]))
    Dispersal_prox[t] = prox_m
    Dispersal_step[t] = step_m
    
    Days_Step[t] = as.numeric(new_dat$Date[t]-new_dat$Date[t-1])
    Days_Rel[t] = as.numeric(new_dat$Date[t]-new_dat$Date[1])
    
  }
  T_LC_Prox = append(T_LC_Prox,Dispersal_prox)
  T_LC_Step = append(T_LC_Step,Dispersal_step)
  
  T_Days_Step = append(T_Days_Step,Days_Step)
  T_Days_Rel = append(T_Days_Rel,Days_Rel)
}
T_LC_Prox
T_LC_Step
T_Days_Rel
T_Days_Step

End = Sys.time()

difftime(End,Start, units = "mins")
```

Here I just add the data to the datafram via cbind which I will then work with later. 
```{r}

T_Manuals_All = cbind(T_LC_Prox, T_Manuals_All)
colnames(T_Manuals_All)[1]<-"LC_Prox_m"#least-cost meters proximity to release ramp

T_Manuals_All = cbind(T_LC_Step, T_Manuals_All)
colnames(T_Manuals_All)[1]<-"LC_Step_m"#least cost step length in meters

T_Manuals_All = cbind(T_Days_Rel, T_Manuals_All)
colnames(T_Manuals_All)[1]<-"Days_Since_Rel"#Days since fish was initial released

T_Manuals_All = cbind(T_Days_Step, T_Manuals_All)
colnames(T_Manuals_All)[1]<-"Days_Step"#Days between previous location and current location for which the step length was calculated

T_Manuals_All$LC_M_D = T_Manuals_All$LC_Step_m/T_Manuals_All$Days_Step

T_Manuals_All$Weeks_Since_Rel = T_Manuals_All$Days_Since_Rel/7

plot(T_Manuals_All$LC_Step_m~T_Manuals_All$Weeks_Since_Rel)

#THIS IS HOW TO SAVE THE SPATIAL GEOMETRY BACK INTO A CSV. 
#st_write(Dispersal,"DispersalTESTER.csv", layer_options = "GEOMETRY=AS_XY")
```

Add graphs about proximity, movement speeds, etc. over time. (by species)? 

## Step Selection (Dispersal Efficiency)

## Distance displaced by tournaments

All we need for this is to filter the main data frame to the row before the fish was caught (its last known location before being caught) and then the row with the ramp it was translocated to. 

```{r}
DistanceTransloc = TrackMaster %>% 
                    filter(RewardTag1 %in% RewardTag1[TournamentRelease=="Y"])
```

This should lead the filter by 1 row thus giving us its last known location and then every location after that.
```{r}
DistanceTransloc = DistanceTransloc %>% 
                   group_by(RewardTag1) %>% 
                   filter(TournamentRelease == "Y" | lead(TournamentRelease=="Y"))
```

Now, all we need is the first two observations to calculate how far these fish were translocated from their original position by tournament anglers
```{r}
DistanceTransloc = DistanceTransloc %>% 
                   group_by(RewardTag1) %>%
                   slice(1:2)
```

This loop should return the relative distance that each fish was translocated to a tournament weigh in site in meters...
```{r}
tag_nums = unique(DistanceTransloc$RewardTag1)
n_fish = length(tag_nums)# of individual fish making up dataset
T_Transloc_Dist = numeric()
#Outside loop that selects tag-number 
for(i in 1:n_fish){
  
  new_tag = tag_nums[i]
  new_dat = subset(DistanceTransloc,RewardTag1 == new_tag)
  tmax = nrow(new_dat)
  Transloc_Dist = numeric(tmax)
  days = numeric(tmax)
  
  for(t in 2:tmax){
    
    dist_m = geosphere::distm(c(new_dat$Long[t],new_dat$Lat[t]),c(new_dat$Long[t-1],new_dat$Lat[t-1]))#
    Transloc_Dist[t] = dist_m
    
  }
  
  T_Transloc_Dist = append(T_Transloc_Dist,Transloc_Dist)
}
T_Transloc_Dist#this is in meters
```

```{r}
T_Transloc_Dist = T_Transloc_Dist[T_Transloc_Dist!=0]

T_Transloc_Dist_KM = T_Transloc_Dist/1000

DistanceTransloc = DistanceTransloc %>% group_by(RewardTag1) %>% slice(2)

DistanceTransloc = cbind(DistanceTransloc,T_Transloc_Dist_KM)

colnames(DistanceTransloc)[26]="T_Transloc_Dist_KM"
```

Plot of translocated distance distribution (by species)
```{r}
ALB_Transloc = filter(DistanceTransloc, Species == "ALB")

h1 = hist(DistanceTransloc$T_Transloc_Dist_KM, breaks = c(seq(0,26,2)),
          xlab = "Distance Translocated By Angler (km)",
          ylab = "# of fish",col="Red")

h2 = hist(ALB_Transloc$T_Transloc_Dist_KM, breaks = c(seq(0,26,2)),col = "Blue")

plot(h1, col = "Red", xlab = "KM Translocated by Angler",main=NULL)
plot(h2, col = "Blue", add=T)
legend(22,10,c("LMB", "ALB"), fill = c("Red","Blue"), title = "Species")
#axis(1,h1$mids, labels = c("0-2", "2.1-4", "4.1,6","6.1,8", #"8.1-10","10.1-12","12.1-14","14.1-16","16.1-18","18.1-20","20.1-22","22.1-24","24.1-26"), tick = FALSE, padj=-1.5)
```

## Homing

This returns all locations for a fish that was AT ANY POINT caught and released in a tournament. 
```{r}
Homing = TrackMaster %>% 
             group_by(RewardTag1) %>%
             filter(any(TournamentRelease == "Y")) %>%
             ungroup
```

This should lead the filter by 1 row thus giving us its last known location and then every location after that.
```{r}
Homing = Homing %>% 
         group_by(RewardTag1) %>% 
         filter(TournamentRelease == "Y" | lead(TournamentRelease=="Y"))
```

Now we need to setup a loop that calculates the relative distance between each successive location and its last known position before it was translocated by an angler... We essentially did this for the proximity to boat ramp loop.

Setup vectors and loop here 
```{r}

tag_nums = unique(Homing$RewardTag1)
n_fish = length(tag_nums)# of individual fish making up dataset

WeeksSinceRel = numeric()
Homing_SL = numeric()

#Outside loop that selects tag-number 
for(i in 1:n_fish){
  
  new_tag = tag_nums[i]
  new_dat = subset(Homing,RewardTag1 == new_tag)
  tmax = nrow(new_dat)
  Homing_Dist = numeric(tmax)
  WeeksFromRel = numeric(tmax)
  
  for(t in 2:tmax){
    
    dist_m = geosphere::distm(c(new_dat$Long[t],new_dat$Lat[t]),c(new_dat$Long[1],new_dat$Lat[1]))#
    Homing_Dist[t] = dist_m
    WeeksFromRel[t] = as.numeric(new_dat$Date[t]-new_dat$Date[2])/7
    
  }
  Homing_SL = append(Homing_SL,Homing_Dist)
  WeeksSinceRel = append(WeeksSinceRel, WeeksFromRel)
}

Homing = cbind(Homing,Homing_SL)
colnames(Homing)[27]<-"ProxMToLastKnownLocation"

Homing = cbind(Homing,WeeksSinceRel)
colnames(Homing)[28]<-"WeeksSinceRel"

Homing
```

Graphs, NEED TO MAKE SURE THERE IS SOME INDICATION WHETHER THEY ARE ALIVE OR NOT. Add POINTS FOR DETECTIONS, PERCENTAGES, SPECIES,  
```{r}

Homing = Homing %>% group_by(RewardTag1) %>% slice(2:n())#Gets rid of double zeroes and intial location

ggplot(data = Homing, aes(x = WeeksSinceRel, y = ProxMToLastKnownLocation/1000, group = RewardTag1,
                          color = as.factor(RewardTag1)))+
  geom_line()+
  geom_point(shape = factor(Homing$GroupMovement))
#triangle = Dead.... Need to look at this panel by panel to look at potential homing.

ggplot(data = Homing, aes(x = WeeksSinceRel, y = ProxMToLastKnownLocation/1000,
                          color = as.factor(RewardTag1)))+
  geom_line()+
  geom_point(shape = factor(Homing$GroupMovement))+
  facet_wrap(vars(RewardTag1))
```

## Alive vs Dead Movement

```{r}
AliveDeadMovement = subset(TrackMaster,ManuallyTracked == "Y")
AliveDeadMovement = AliveDeadMovement %>% group_by(RewardTag1) %>% filter(n()>1)
```

```{r}
tag_nums = unique(AliveDeadMovement$RewardTag1)
n_fish = length(tag_nums)# of individual fish making up dataset

AliveDead_SL = numeric()
AliveDead_weeks = numeric()#only need to calculate this one time... So, this vector will not be included in the second loop
```

```{r}
#Outside loop that selects tag-number 
for(i in 1:n_fish){
  
  new_tag = tag_nums[i]
  new_dat = subset(AliveDeadMovement,RewardTag1 == new_tag)
  tmax = nrow(new_dat)
  AliveDead_Dist = numeric(tmax)
  weeks = numeric(tmax)
  
  for(t in 2:tmax){
    
    dist_m = geosphere::distm(c(new_dat$Long[t],new_dat$Lat[t]),c(new_dat$Long[t-1],new_dat$Lat[t-1]))#
    AliveDead_Dist[t] = dist_m
    weeks[t] = as.numeric(new_dat$Date[t]-new_dat$Date[t-1])/7
  }
  AliveDead_SL = append(AliveDead_SL,AliveDead_Dist)
  AliveDead_weeks = append(AliveDead_weeks,weeks)
}
```

cbinding the data to the dataframe
```{r}
AliveDeadMovement = cbind(AliveDeadMovement,AliveDead_SL)
colnames(AliveDeadMovement)[27] <- "SL_Movement_M"

AliveDeadMovement = cbind(AliveDeadMovement,AliveDead_weeks)
colnames(AliveDeadMovement)[28] <- "WeeksFromLast"

AliveDeadMovement$meters_per_week = AliveDeadMovement$SL_Movement_M/AliveDeadMovement$WeeksFromLast

AliveDeadMovement = AliveDeadMovement %>% group_by(RewardTag1) %>% slice(2:n())
```

```{r}
ggplot(AliveDeadMovement, aes(x=GroupMovement, y = meters_per_week))+
  geom_boxplot()+
  #scale_y_continuous(limits = c(0,200))+
  theme_classic()
```

## Assigning lake sections

This should assign the SectionNum for each point in our dataset. 1 is closest to the dam, 6 is up the river section. 
```{r}
Sections = sf::st_as_sf(TrackMaster, coords = c("Long","Lat"),crs = crs(NeelySHP))

SectionAssignment = st_join(Sections, NeelySections)
```



## Tagging data to inform loading rate

## Average dispersal rates inform how quickly loaded fish move out of CL Zone

## Percents of tagged fish in zone at any given time.#Spatial Redistribution

```{r}
#THIS IS HOW TO SAVE THE SPATIAL GEOMETRY BACK INTO A CSV. 
#st_write(Dispersal,"DispersalTESTER.csv", layer_options = "GEOMETRY=AS_XY")
```

######################################################################################
######################################################################################
######################################################################################

###### MAKE GRAPHS HERE 
###### PERCENT WITHIN 3 km over time

######################################################################################
######################################################################################
######################################################################################








######################################################################################
######################################################################################
######################################################################################

# Model to Evaluate Loading of Fish Around CL

Here I am going to cleanup the data to help deal with the loading rate of fishes to Coosa Landing. To do so I need to summarize the tag-return data on a monthly basis. Additionally, we will have to account for non-reporting either due through angler non-reporting, tag loss, and fish that have already been caught that are UNABLE to be reported due to already having their tags removed by anglers. 

Now we need to address the few random ones in the dataset such as UNK, "March", "2 weeks ago"

##########################################
##########################################
##########################################


This code assigns the correct reward level to the fish that were returned by anglers. I will do the same for the tagging data.

```{r}
TaggingMaster = read.csv("https://github.com/PrullMarcus/MastersAnalysis/raw/main/TaggingMasterSheet.csv")
```

```{r}
TaggingMaster$Reward.Tag.. = as.numeric(TaggingMaster$Reward.Tag..)

TaggingMaster = TaggingMaster %>% 
            mutate(RewardLevel = case_when(
              Reward.Tag.. <= 1000 | Reward.Tag.. >=4000 & Reward.Tag..<=4499 ~ 1,
              Reward.Tag.. >=1001 & Reward.Tag..<=2000 | Reward.Tag.. >=5000 & Reward.Tag..<=5249 ~ 2,
              Reward.Tag.. >=2001 & Reward.Tag..<=2500 ~ 3
            ))
TaggingMaster
```

We need # tagged by period (Month/Year) separated by reward value and tagging status (single/double) 
Currently the "Date" Column is not actually being stored as a data. Its being stored as a "chr". We are gonna use lubridate to make an actual date column. THEN, we need to make a "month/year which will set the basis of our matrix we want to create. 

```{r}
TaggingMaster$Date = as.Date(TaggingMaster$Date, format = "%m/%d/%y")
TaggingMaster$MonthYear = format(TaggingMaster$Date, "%m-%Y")
unique(TaggingMaster$MonthYear)
```

Creating the periods string (first tagging period till end of year 2023). 
```{r} 
Period_Seq = format(seq.Date(as.Date("2022-01-01"), as.Date("2023-12-01"), by = "month"),format = "%m-%Y")
```

Okay, we need first column is the tag type, second column is release period, second column is total tagged, and then every successive column is the recaps for each tagging period. 

Reward # Tag   RelPeriod  #Tagged 12/23   1/24   2/24   3/24   4/24   
100    Sing       1/22       10     0      0      0          
100    Doub       1/22       20     0      0      0     
200    Sing       1/22      ...    ...    ...    ... 
200    Doub       1/22      ...    ...    ...    ...
300    Sing       1/22      ...    ...    ...    ...
100    Sing       2/22       20     0      0      0          
100    Doub       2/22       35     0      0      0     
200    Sing       2/22      ...    ...    ...    ... 
200    Doub       2/22      ...    ...    ...    ...
300    Sing       2/22      ...    ...    ...    ...

```{r}
test = TaggingMaster %>% group_by(MonthYear, RewardLevel, Num.tagged) %>% count(n())
#test = tidyr::pivot_wider(test,names_from = MonthYear, values_from=n)
test[is.na(test)]=0
test
```


```{r}
MonthYearOrder = c("01-2022", "02-2022", "12-2022","01-2023","05-2023")
RewardLevOrder = c(1,2,3)
SingleDoubleOrder = c(1,2)
```

```{r}
test$MonthYear = factor(test$MonthYear, levels = MonthYearOrder)

test = test[with(test,order(factor(test$MonthYear, levels = MonthYearOrder),factor(test$RewardLevel, levels = RewardLevOrder),factor(test$Num.tagged, levels = SingleDoubleOrder))),]

test
```
Now we need to somehow figure out how to tell r where to assign all of our tag returns. We need to take the tag return data, assign every tag return to its proper release period, reward group, and capture month. Then have r take a tally of all the fish in each criteria/recap group. 

This gets rid of the n() column which is just the total of all the tag types.  
```{r}
test = test[,!(names(test) %in% "n()")]
test
```

Firstly, I am going to take the tag recap matrix and make the capture date into a month/year format like we have in our matrix. Additionally, I am going to add its release period and TaggingGroup which we can figure out from the tagging data for that individual fish. 
```{r}
tag_return = read.csv("https://github.com/PrullMarcus/MastersAnalysis/raw/main/TagReturnsMasterSheet.csv")
```

```{r}
tourney_returns = filter(tag_return, If.released.it.was == "c")
tourney_returns
```
Selecting only variables I think we need for this. 

```{r}
tourney_returns = tourney_returns %>% dplyr::select(c("Tag.number","Second.Tag","Date.of.catch","Which.body.of.water",
                                   "Access.to.the.lake","If.released.it.was"))
tourney_returns
```

An easy thing to do first is to change the Date.of.catch part to MonthYear just like it is in the matrix we setup.
```{r}
tourney_returns$Date.of.catch = as.Date(tourney_returns$Date.of.catch, format = "%m/%d/%y")
tourney_returns$RecapMonthYear = format(tourney_returns$Date, "%m-%Y")
unique(tourney_returns$RecapMonthYear)
tourney_returns
```

Now have the release date in the same format as the matrix. Now we need to add whether the fish was single/double tagged intitially. Then we need to combine the assigned reward to the fish as well as its release month. Then we should be able to populate the matrix correctly.  
```{r}
tmp1 <- merge(tourney_returns, TaggingMaster, by.x = "Tag.number", by.y = "Reward.Tag..", all.x = FALSE)
tmp1 = tmp1 %>% dplyr::select(c("Tag.number","Second.Tag","Which.body.of.water",
                                   "Access.to.the.lake", "MonthYear",
                                   "RecapMonthYear","Num.tagged", "RewardLevel"))

tmp2 <- merge(tourney_returns, TaggingMaster, by.x = "Tag.number", by.y = "X2nd.Reward.Tag..", all.x = FALSE)
tmp2 = tmp2 %>% dplyr::select(c("Tag.number","Second.Tag","Which.body.of.water",
                                   "Access.to.the.lake", "MonthYear",
                                  "RecapMonthYear","Num.tagged", "RewardLevel"))

tmp3 = rbind(tmp1,tmp2)
tmp3
```

Missing two fish?????

```{r}
test2 = tmp3 %>% group_by(MonthYear, RewardLevel,Num.tagged, RecapMonthYear) %>% count(n())
test2 = tidyr::pivot_wider(test2,names_from = RecapMonthYear, values_from=n)
test2[is.na(test2)]=0
test2
```
```{r}
test2 = test2[,!(names(test2) %in% "n()")]
test2
```

```{r}

non_date_cols = c("MonthYear", "RewardLevel","Num.tagged")
date_cols = setdiff(colnames(test2), non_date_cols)
missing_months = setdiff(Period_Seq, date_cols)
for(m in missing_months){
  
  test2[[m]]<-0
  
}

test2 = test2[,c(non_date_cols, Period_Seq)]
test2
```


Now I need to sort them properly (tag types in proper order and release periods in order)
```{r}

test2 = test2[with(test2,order(factor(test2$MonthYear, levels = MonthYearOrder),factor(test2$RewardLevel, levels = RewardLevOrder),factor(test2$Num.tagged, levels = SingleDoubleOrder))),]
test2
```

I just simplified down the original test dataset to be merged with our recap matrix. 
```{r}
tests_merged = test %>% left_join(test2, by = c("MonthYear", "RewardLevel","Num.tagged"))
tests_merged
```
Now I just need to replace the NAs with 0s 
```{r}
tests_merged = tests_merged %>% mutate(across(where(is.numeric),~replace_na(.,0)))
tests_merged
```

############  estimation model  ###################

```{r}

nll=function(theta,recaps_hi_sing,recaps_med_sin,recaps_med_doub,recaps_low_sin,recaps_low_doub,TL){
  cm=plogis(theta[1])
  
  lambda=plogis(theta[2:3])
  
  u=plogis(theta[4:(n_yrs+3)])
  S=matrix(NA,nrow=n_rel,ncol=n_yrs)
  
  p_ret_hi_sin=matrix(0,nrow=n_rel,ncol=n_yrs+1)
  
  p_ret_med_sin=matrix(0,nrow=n_rel,ncol=n_yrs+1)
  p_ret_med_doub=matrix(0,nrow=n_rel,ncol=n_yrs*2+1)
  
  p_ret_low_sin=matrix(0,nrow=n_rel,ncol=n_yrs+1)
  p_ret_low_doub=matrix(0,nrow=n_rel,ncol=n_yrs*2+1)
  
  nll_hi_sin=numeric(n_rel)#never double tagged $300 fish
  
  nll_med_sin=numeric(n_rel)
  nll_med_doub=numeric(n_rel)
  
  nll_low_sin=numeric(n_rel)
  nll_low_doub=numeric(n_rel)
  
  
  for (r in 1:n_rel){
    for(y in r:n_yrs){
      S[r,y]=(1-u[y])*(1-cm)
      if(r==y){
        p_ret_hi_sin[r,y]=u[y]*(1-TL)
        
        p_ret_med_sin[r,y]=u[y]*(1-TL)*lambda[1]
        p_ret_med_doub[r,y*2-1]=u[y]*2*TL*(1-TL)*lambda[1]
        p_ret_med_doub[r,y*2]=u[y]*(1-TL)^2*lambda[1]

        
        p_ret_low_sin[r,y]=u[y]*(1-TL)*lambda
        p_ret_low_doub[r,y*2-1]=u[y]*2*TL*(1-TL)*lambda[2]
        p_ret_low_doub[r,y*2]=u[y]*(1-TL)^2*lambda[2]
    
      } else {
        
        p_ret_hi_sin[r,y]=u[y]*(1-TL)*prod(S[r,r:(y-1)])
        
        p_ret_med_sin[r,y]=u[y]*(1-TL)*lambda[1]*prod(S[r,r:(y-1)])
        p_ret_med_doub[r,y*2-1]=u[y]*2*TL*(1-TL)*lambda[1]*prod(S[r,r:(y-1)])
        p_ret_med_doub[r,y*2]=u[y]*(1-TL)^2*lambda[1]*prod(S[r,r:(y-1)])

        
        p_ret_low_sin[r,y]=u[y]*(1-TL)*lambda*prod(S[r,r:(y-1)])
        p_ret_low_doub[r,y*2-1]=u[y]*2*TL*(1-TL)*lambda[2]*prod(S[r,r:(y-1)])
        p_ret_low_doub[r,y*2]=u[y]*(1-TL)^2*lambda[2]*prod(S[r,r:(y-1)])
        
      }  # close if else
      
    }  #close y loop
    p_ret_hi_sin[r,n_yrs+1]=1-sum(p_ret_hi[r,])
    
    p_ret_med_sin[r,n_yrs+1]=1-sum(p_ret_med_sin[r,])
    p_ret_med_doub[r,n_yrs+1]=1-sum(p_ret_med_doub[r,])
    
    p_ret_low_sin[r,n_yrs+1]=1-sum(p_ret_low_sin[r,])
    p_ret_low_doub[r,n_yrs+1]=1-sum(p_ret_low_doub[r,])

    nll_hi_sin[r]=-dmultinom(recaps_hi_sin[r,],prob=p_ret_hi_sin[r,],log=T)
    
    nll_med_sin[r]=-dmultinom(recaps_med_doub[r,],prob=p_ret_med_doub[r,],log=T)
    nll_med_doub[r]=-dmultinom(recaps_med_doub[r,],prob=p_ret_med_doub[r,],log=T)
    
    nll_low_sin[r]=-dmultinom(recaps_low_sin[r,],prob=p_ret_low_sin[r,],log=T)
    nll_low_doub[r]=-dmultinom(recaps_low_doub[r,],prob=p_ret_low_doub[r,],log=T)

  } #close r loop
  sum(nll_hi_sin)+sum(nll_med_sin)+sum(nll_med_doub)+sum(nll_low_sin)+sum(nll_low_doub)
}  #close function

theta=qlogis(c(.3,lambda = c(.9,.8),u = rep(0.1,n_yrs)))

nll(theta,recaps_hi_sin,recaps_med_sin,recaps_med_doub,recaps_low_sin,recaps_low_doub)



#####STOPPED HERE. 




fit=optim(theta,fn=nll,method='BFGS',hessian=T,recaps_hi=recaps_hi,recaps_low=recaps_low,TL=TL)

plogis(fit$par)
plogis(theta)

se=sqrt(diag(solve(fit$hessian)))
u95ci=plogis(fit$par+1.96*se)
l95ci=plogis(fit$par-1.96*se)
u95ci
plogis(fit$par)
l95ci


nsamps=sum(recaps_hi)+sum(recaps_low)
aicc=2*fit$value+2*length(fit$par)*(length(fit$par)+1)/(nsamps-length(fit$par)-1)

```

```{r}

```

HERE IS THE MODEL WITHOUT DIFFERENCIATING BETWEEN TAG TYPES... 
```{r}
n_yrs = 24#number of recap periods. 2 calendar years with a montly timestep = 24 months.
n_rel = nrow(tests_merged)#Do we need to change this to 5 if we aren't differenciating by tag-type within the model????
TL = 0.05
```

```{r}
tests_merged$NotSeen = tests_merged$n-apply(tests_merged[,5:27],1,sum)
```

```{r}

my_released=my(tests_merged$MonthYear)#converts to date
release_period=interval(my("1-2022"),my_released) %/% months(1)+1

```
estimate 4 cms, vector that is n_yrs long, 1, 2, 3, 4, 4 cms, 1 for each season. cm is all mortalities.


```{r}

nll=function(theta,recaps,TL,nTag,RewardLevel,release_period){
 
  cm=plogis(theta[1])
  
  lambda=plogis(theta[2:3])
  lambda=c(lambda,1)
  
  u=plogis(theta[4:(n_yrs+3)])
  S=matrix(NA,nrow=n_rel,ncol=n_yrs)
  
  p_ret=matrix(0,nrow=n_rel,ncol=n_yrs+1)
  nll=numeric(n_rel)

  for (r in 1:n_rel){
    for(y in release_period[r]:n_yrs){
      S[r,y]=(1-u[y])*(1-cm)
      if(release_period[r]==y){
        
        p_ret[r,y]=u[y]*(1-TL^nTag[r])*lambda[RewardLevel[r]]
        
      } else {
        
        p_ret[r,y]=u[y]*(1-TL^nTag[r])*prod(S[r,release_period[r]:(y-1)])*lambda[RewardLevel[r]]
        
      }  # close if else
      
    }  #close y loop
    
    p_ret[r,n_yrs+1]=1-sum(p_ret[r,])
    nll[r]=-dmultinom(recaps[r,],prob=p_ret[r,],log=T)
    
  } #close r loop
  sum(nll)
}  #close function

theta=qlogis(c(.3,lambda = c(.8,.9),u = rep(0.1,n_yrs)))

nll(theta,TL=TL,recaps = tests_merged[,5:29],nTag = tests_merged$Num.tagged, RewardLevel = tests_merged$RewardLevel,release_period=release_period)

#####STOPPED HERE. 

fit=optim(theta,fn=nll,method='BFGS',hessian=T,TL=TL,nTag=nTag,RewardLevel=RewardLevel,release_period=release_period,recaps=recaps)



plogis(fit$par)
plogis(theta)

se=sqrt(diag(solve(fit$hessian)))
u95ci=plogis(fit$par+1.96*se)
l95ci=plogis(fit$par-1.96*se)
u95ci
plogis(fit$par)
l95ci


nsamps=sum(recaps_hi)+sum(recaps_low)
aicc=2*fit$value+2*length(fit$par)*(length(fit$par)+1)/(nsamps-length(fit$par)-1)

```